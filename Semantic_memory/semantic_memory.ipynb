{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -r requirements.txt "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import shutil\n",
    "import requests\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv(\"credentials.env\")\n",
    "from azure.core.credentials import AzureKeyCredential\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "endpoint = os.getenv(\"AZURE_SEARCH_ENDPOINT\")\n",
    "credential = os.getenv(\"AZURE_SEARCH_KEY\")\n",
    "azure_openai_endpoint = os.getenv(\"AZURE_OPENAI_EMBEDDINGS_ENDPOINT\")\n",
    "openai_api_version = os.getenv(\"AZURE_OPENAI_API_VERSION\")\n",
    "azure_openai_apikey = os.getenv(\"AZURE_OPENAI_API_KEY\")\n",
    "credential = AzureKeyCredential(credential)\n",
    "openai_api_key=os.getenv(\"OPENAI_API_KEY\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -qU langchain-openai\n",
    "%pip install -qU langchain\n",
    "%pip install -qU BeautifulSoup4\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "vscode": {
     "languageId": "ruby"
    }
   },
   "outputs": [],
   "source": [
    "from langchain import hub\n",
    "from langchain.agents import AgentExecutor, create_react_agent\n",
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.tools import Tool\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "vscode": {
     "languageId": "ruby"
    }
   },
   "outputs": [],
   "source": [
    "from langchain.callbacks.base import BaseCallbackHandler\n",
    "##Callback for seeing the realtime thinking process\n",
    "class RealTimeCallbackHandler(BaseCallbackHandler):\n",
    "    def on_tool_start(self, serialized, input_str, **kwargs):\n",
    "        print(f\"Agent is thinking: {input_str}\")\n",
    "    \n",
    "    def on_tool_end(self, output, **kwargs):\n",
    "        print(f\"Agent received response: {output}\")\n",
    "    \n",
    "    def on_tool_error(self, error, **kwargs):\n",
    "        print(f\"An error occurred: {error}\")\n",
    "    \n",
    "    def on_agent_action(self, action, **kwargs):\n",
    "        print(f\"Agent action: {action.tool} with input: {action.tool_input}\")\n",
    "    \n",
    "    def on_agent_finish(self, finish, **kwargs):\n",
    "        print(\"Agent has finished thinking.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "vscode": {
     "languageId": "ruby"
    }
   },
   "outputs": [],
   "source": [
    "# Choose the LLM to use\n",
    "llm = ChatOpenAI()\n",
    "# Construct the ReAct agent\n",
    "prompt = hub.pull(\"hwchase17/react\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "vscode": {
     "languageId": "ruby"
    }
   },
   "outputs": [],
   "source": [
    "from langchain.pydantic_v1 import BaseModel, Field\n",
    "from langchain.tools import BaseTool\n",
    "from langchain.utilities import BingSearchAPIWrapper\n",
    "from typing import Type, Optional\n",
    "from langchain.callbacks.manager import AsyncCallbackManagerForToolRun, CallbackManagerForToolRun\n",
    "import asyncio\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "class SearchInput(BaseModel):\n",
    "    query: str = Field(description=\"Should be a search query\")\n",
    "\n",
    "class MyBingSearch(BaseTool):\n",
    "    \"\"\"Tool for a Bing Search Wrapper\"\"\"\n",
    "\n",
    "    # Add type annotations for all overridden attributes\n",
    "    name: str = \"Searcher\"\n",
    "    description: str = \"Useful for searching the internet.\"\n",
    "    args_schema: Type[BaseModel] = SearchInput\n",
    "\n",
    "    k: int = 5  # Number of search results to return\n",
    "\n",
    "    def _run(self, query: str, run_manager: Optional[CallbackManagerForToolRun] = None) -> str:\n",
    "        \"\"\"Synchronous Bing search.\"\"\"\n",
    "        bing = BingSearchAPIWrapper(k=self.k)\n",
    "        return bing.results(query, num_results=self.k)\n",
    "\n",
    "    async def _arun(self, query: str, run_manager: Optional[AsyncCallbackManagerForToolRun] = None) -> str:\n",
    "        \"\"\"Asynchronous Bing search.\"\"\"\n",
    "        bing = BingSearchAPIWrapper(k=self.k)\n",
    "        loop = asyncio.get_event_loop()\n",
    "        results = await loop.run_in_executor(ThreadPoolExecutor(), bing.results, query, self.k)\n",
    "        return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "vscode": {
     "languageId": "ruby"
    }
   },
   "outputs": [],
   "source": [
    "def parse_html(content) -> str:\n",
    "    soup = BeautifulSoup(content, 'html.parser')\n",
    "    text_content_with_links = soup.get_text()\n",
    "    words = text_content_with_links.split()\n",
    "    first_100_words = ' '.join(words[:20])\n",
    "    text_content_with_links = first_100_words\n",
    "    return text_content_with_links\n",
    "\n",
    "def fetch_web_page(url: str) -> str:\n",
    "    HEADERS = {'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10.15; rv:90.0) Gecko/20100101 Firefox/90.0'}\n",
    "    response = requests.get(url, headers=HEADERS)\n",
    "    return parse_html(response.content)\n",
    "\n",
    "\n",
    "web_fetch_tool = Tool.from_function(\n",
    "    func=fetch_web_page,\n",
    "    name=\"WebFetcher\",\n",
    "    description=\"useful to fetch the content of a url\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "vscode": {
     "languageId": "ruby"
    }
   },
   "outputs": [],
   "source": [
    "from langchain.agents import create_react_agent, AgentExecutor\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.callbacks.manager import CallbackManager\n",
    "\n",
    "# Initialize the tools (replace MyBingSearch and web_fetch_tool with your actual tools)\n",
    "tools = [MyBingSearch(k=5), web_fetch_tool]\n",
    "\n",
    "# Instantiate the callback handler\n",
    "callback_handler = RealTimeCallbackHandler()\n",
    "callback_manager = CallbackManager([callback_handler])\n",
    "\n",
    "# Construct the OpenAI Tools agent\n",
    "agent = create_react_agent(llm, tools, prompt)\n",
    "\n",
    "# Create the agent executor with the callback manager\n",
    "agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=False, \n",
    "                               return_intermediate_steps=True, callback_manager=callback_manager, handle_parsing_errors=True)\n",
    "\n",
    "# Invoke the agent and capture the result\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "ruby"
    }
   },
   "outputs": [],
   "source": [
    "# Initialize storage for iteration log\n",
    "iteration_log = {}\n",
    "iteration_number = 1\n",
    "question_itr = 0\n",
    "\n",
    "###Some questions for populating the cosmosdb database. The script will the prompt the user with y or n, if the user is satisfied with the answer type \"y\" if not type \"n\". \n",
    "questions = [\"Most recent album published by Tyler the Creator\", \n",
    "\"When is the Microsoft Ignite conference?\", \n",
    "\"Who was appointed as the new Secretary of Defense in the latest U.S. administration?\", \n",
    "\"Who won the Nobel Peace Prize in 2024, and for what contributions?\", \n",
    "\"When did the United Nations announce its latest climate change report, and what were its main findings?\", \n",
    "\"Who has been elected the new President of the United States, and what are the implications of this election?\",\n",
    "\"How has the recent U.S. presidential election, resulting in Donald Trump's victory, impacted international diplomatic relations and global market dynamics?\"\n",
    "    \n",
    "]\n",
    "\n",
    "# Loop through questions\n",
    "while question_itr < len(questions):\n",
    "    question = questions[question_itr]\n",
    "    output = None\n",
    "\n",
    "    # Attempt to invoke the agent and retrieve the answer\n",
    "    while output is None:\n",
    "        try:\n",
    "            print(f\"Question {question_itr + 1}: {question}\")\n",
    "            output = agent_executor.invoke({\"input\": question})\n",
    "            print(\"Agent's answer:\", output[\"output\"])\n",
    "        except Exception as e:\n",
    "            print(f\"Error occurred: {e}. Retrying...\")\n",
    "\n",
    "    # Prompt the user for satisfaction with the answer\n",
    "    user_input = input(\"Are you satisfied with the answer? (y/n): \").strip().lower()\n",
    "\n",
    "    if user_input == 'y':\n",
    "        print(\"Thank you for your feedback. Moving to the next question.\")\n",
    "        iteration_log[iteration_number] = {\n",
    "            \"question\": question,\n",
    "            \"agent_actions\": [(step.tool, step.tool_input, step.log) for step, _ in output.get(\"intermediate_steps\", [])],\n",
    "            \"answer\": output.get(\"output\", \"No output available\"),\n",
    "            \"user_satisfaction\": user_input\n",
    "        }\n",
    "        iteration_number += 1\n",
    "        question_itr += 1\n",
    "    elif user_input == 'n':\n",
    "        print(\"Thank you. Your feedback has been noted.\")\n",
    "        iteration_log[iteration_number] = {\n",
    "            \"question\": question,\n",
    "            \"agent_actions\": [(step.tool, step.tool_input, step.log) for step, _ in output.get(\"intermediate_steps\", [])],\n",
    "            \"answer\": output.get(\"output\", \"No output available\"),\n",
    "            \"user_satisfaction\": user_input\n",
    "        }\n",
    "        iteration_number += 1\n",
    "        question_itr += 1  # You can choose whether to retry the question or move to the next\n",
    "    else:\n",
    "        print(\"Invalid input. Please enter 'y' for yes or 'n' for no.\")\n",
    "        continue\n",
    "\n",
    "# Print the stored log for review\n",
    "print(\"\\nFinal Log:\", iteration_log)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "vscode": {
     "languageId": "ruby"
    }
   },
   "outputs": [],
   "source": [
    "'''\n",
    "In this, we will store the user feedback for further offline self reflection in cosmosDB\n",
    "\n",
    "'''\n",
    "import os\n",
    "import random\n",
    "from langchain_community.chat_message_histories import ChatMessageHistory, CosmosDBChatMessageHistory\n",
    "from langchain_core.chat_history import BaseChatMessageHistory\n",
    "from langchain_core.runnables import ConfigurableFieldSpec\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "from azure.cosmos import CosmosClient, PartitionKey\n",
    "# Initialize the CosmosDB client\n",
    "\n",
    "def store_final_log_in_cosmos(session_id: str, user_id: str, final_log: dict) -> None:\n",
    "    # Configure Cosmos DB connection\n",
    "    cosmos_endpoint = os.environ['AZURE_COSMOSDB_ENDPOINT']\n",
    "    cosmos_database = os.environ['AZURE_COSMOSDB_NAME']\n",
    "    cosmos_container = os.environ['AZURE_COSMOSDB_CONTAINER_NAME']\n",
    "    cosmos_connection_string = os.environ['AZURE_COSMOSDB_CONNECTION_STRING']\n",
    "\n",
    "    # Initialize Cosmos DB client\n",
    "    client = CosmosClient.from_connection_string(cosmos_connection_string)\n",
    "\n",
    "    # Get database and container\n",
    "    database = client.get_database_client(cosmos_database)\n",
    "    container = database.get_container_client(cosmos_container)\n",
    "\n",
    "    # Prepare the data to store\n",
    "    document = {\n",
    "        \"id\": session_id,  # Unique identifier for the record\n",
    "        \"user_id\": user_id,\n",
    "        \"final_log\": final_log\n",
    "    }\n",
    "\n",
    "    # Store the data in the container\n",
    "    container.upsert_item(document)\n",
    "    print(\"Final log successfully stored in Cosmos DB.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "vscode": {
     "languageId": "ruby"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final log successfully stored in Cosmos DB.\n"
     ]
    }
   ],
   "source": [
    "store_final_log_in_cosmos(session_id=\"session_12345\", user_id=\"user_678900\", final_log=iteration_log)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "ruby"
    }
   },
   "outputs": [],
   "source": [
    "###Here we will implement the offline learning method using the react agent. Firstly, we will retrieve or stored json data from CosmosDB. \n",
    "\n",
    "##Fetching the data from cosmoDB and passing it on to the model for offile reinforcement learning. We will be using ReACT agent for that too. \n",
    "\n",
    "import os\n",
    "from azure.cosmos import CosmosClient\n",
    "\n",
    "def get_final_log_from_cosmos(session_id: str) -> dict:\n",
    "    # Configure Cosmos DB connection\n",
    "    cosmos_endpoint = os.environ['AZURE_COSMOSDB_ENDPOINT']\n",
    "    cosmos_database = os.environ['AZURE_COSMOSDB_NAME']\n",
    "    cosmos_container = os.environ['AZURE_COSMOSDB_CONTAINER_NAME']\n",
    "    cosmos_connection_string = os.environ['AZURE_COSMOSDB_CONNECTION_STRING']\n",
    "\n",
    "    # Initialize Cosmos DB client\n",
    "    client = CosmosClient.from_connection_string(cosmos_connection_string)\n",
    "\n",
    "    # Get database and container\n",
    "    database = client.get_database_client(cosmos_database)\n",
    "    container = database.get_container_client(cosmos_container)\n",
    "\n",
    "    # Query to get the document by session_id\n",
    "    query = f\"SELECT * FROM c WHERE c.id = '{session_id}'\"\n",
    "    items = list(container.query_items(query=query, enable_cross_partition_query=True))\n",
    "\n",
    "    # Check if any document was retrieved\n",
    "    if not items:\n",
    "        print(f\"No document found for session_id: {session_id}\")\n",
    "        return None\n",
    "\n",
    "    # Assuming session_id is unique, return the first matching document\n",
    "    document = items[0]\n",
    "    return document\n",
    "\n",
    "# Example usage\n",
    "retrieved_log = get_final_log_from_cosmos(session_id=\"session_12345\")\n",
    "\n",
    "if retrieved_log:\n",
    "    print(\"Retrieved Final Log:\")\n",
    "    print(json.dumps(retrieved_log, indent=4))\n",
    "else:\n",
    "    print(\"No log found.\")\n",
    "\n",
    "\n",
    "retrieved_log.get(\"final_log\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "ruby"
    }
   },
   "outputs": [],
   "source": [
    "final_log_offline = retrieved_log.get(\"final_log\")\n",
    "##The log that we got has been stored in the final_log_offline variable. We will now use this log to train the model offline.. \n",
    "final_log_offline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "ruby"
    }
   },
   "outputs": [],
   "source": [
    "questions_offline = []\n",
    "\n",
    "# Extract questions from final_log_offline\n",
    "for key, value in final_log_offline.items():\n",
    "    question = value.get('question')  # Get the 'question' field\n",
    "    if question:  # Check if question exists\n",
    "        questions_offline.append(question)\n",
    "\n",
    "# Print or use the questions_offline array\n",
    "print(\"Questions extracted:\", questions_offline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "ruby"
    }
   },
   "outputs": [],
   "source": [
    "##It came to my attention that whenever we passed error proned chain of thought to the ReACT Agent for offline learning, it would get stuck on the error and not move forward with self reflection. replace_exceptions handles the instance of _Exception by replacing it with Error encountered to get rid of model's ambiguity. \n",
    "def replace_exceptions(data):\n",
    "    \"\"\"\n",
    "    Recursively replace '_Exception' with 'Error encountered' in any data structure.\n",
    "\n",
    "    Parameters:\n",
    "    - data: The data structure (dict, list, str, etc.)\n",
    "\n",
    "    Returns:\n",
    "    - The data structure with '_Exception' replaced.\n",
    "    \"\"\"\n",
    "    if isinstance(data, dict):\n",
    "        return {key: replace_exceptions(value) for key, value in data.items()}\n",
    "    elif isinstance(data, list):\n",
    "        return [replace_exceptions(element) for element in data]\n",
    "    elif isinstance(data, str):\n",
    "        return data.replace('_Exception', 'Error encountered')\n",
    "    else:\n",
    "        return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "ruby"
    }
   },
   "outputs": [],
   "source": [
    "##replaces all the _Exception with Error encountered\n",
    "final_log_offline = replace_exceptions(final_log_offline)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "vscode": {
     "languageId": "ruby"
    }
   },
   "outputs": [],
   "source": [
    "OFFLINE_PROMPT = \"\"\"\n",
    "YOUR FINAL OUTPUT SHOULD HAVE ATLEAST 200 WORDS AND MAKE IT GENERALIZED SO THAT IT CAN BE USED FOR FUTURE TASKS.\n",
    "You have completed the task: '{question}' using the following steps: {agent_actions}. However, the user indicated dissatisfaction ('user_satisfaction' marked as 'n'). Reflect on your approach to identify areas for improvement and develop actionable strategies for future tasks.\n",
    "\n",
    "Focus on these key aspects:\n",
    "\n",
    "### Evaluate Your Approach:\n",
    "- **Logical Flow:** Did your actions logically address the task? Were they aligned with user expectations?\n",
    "- **Action Effectiveness:** Assess the relevance and impact of each action in addressing the task requirements.\n",
    "\n",
    "### Identify Issues and Causes:\n",
    "- **Errors/Blockers:** Recognize any challenges or failed attempts (e.g., resource inaccessibility, formatting errors).\n",
    "- **Root Causes:** Analyze why these issues occurred and whether they stemmed from tool selection, search strategy, or external factors.\n",
    "\n",
    "### Refine Navigation and Resource Use:\n",
    "- **Search Optimization:** How could you improve your search queries or strategies for more relevant results?\n",
    "- **Resource Exploration:** Identify alternative, reliable sources to mitigate errors or access issues.\n",
    "- **Verification:** Emphasize checking resource reliability and accessibility before use.\n",
    "\n",
    "### Enhance Reasoning and Adaptability:\n",
    "- **Adapt to Challenges:** Reflect on how you handled unexpected issues and identify opportunities for greater flexibility.\n",
    "- **Plan Ahead:** Consider whether better planning or foresight could prevent similar issues.\n",
    "\n",
    "### Generalized Principles:\n",
    "- **High-Level Strategies:** Develop adaptable strategies for handling similar tasks effectively in the future.\n",
    "- **Best Practices:** Highlight principles for reasoning, planning, and navigating resources, focusing on reliability and adaptability.\n",
    "- **Preventative Measures:** Suggest ways to avoid recurring issues, ensuring robust and user-aligned task execution.\n",
    "\n",
    "**Structure your response:**\n",
    "1. **Reflection on Performance:** Summarize your reasoning and task alignment.\n",
    "2. **Improvement Insights:** Provide actionable suggestions for refining execution and resource use.\n",
    "3. **Generalized Principles:** Offer strategies to approach similar tasks more effectively, emphasizing adaptability, planning, and navigation.\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "ruby"
    }
   },
   "outputs": [],
   "source": [
    "# Assuming final_log_offline is a dictionary with structure like:\n",
    "# final_log_offline = {\n",
    "#     '1': {'question': 'First question text', 'agent_actions': [...]},\n",
    "#     '2': {'question': 'Second question text', 'agent_actions': [...]},\n",
    "#     ...\n",
    "# }\n",
    "\n",
    "if final_log_offline:\n",
    "    for key, value in final_log_offline.items():\n",
    "        while True:  # Retry loop\n",
    "            try:\n",
    "                # Extract the question and agent actions for the current key\n",
    "                question = value.get(\"question\", \"No question provided.\")\n",
    "                agent_actions = value.get(\"agent_actions\", [])\n",
    "\n",
    "                # Format the OFFLINE_PROMPT with the extracted question and agent actions\n",
    "                formatted_prompt = OFFLINE_PROMPT.format(\n",
    "                    question=question,\n",
    "                    agent_actions=agent_actions\n",
    "                )\n",
    "\n",
    "                # Invoke the agent executor with the formatted prompt\n",
    "                output = agent_executor.invoke({\"input\": formatted_prompt})\n",
    "\n",
    "                # Display the agent's answer\n",
    "                print(f\"Agent's answer for Question {key}:\", output[\"output\"])\n",
    "\n",
    "                # Exit the retry loop if successful\n",
    "                break\n",
    "\n",
    "            except Exception as e:\n",
    "                # Handle and display the error, then retry\n",
    "                print(f\"Error encountered for Question {key}: {e}. Retrying...\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "vscode": {
     "languageId": "ruby"
    }
   },
   "outputs": [],
   "source": [
    "###Let us convert the question into embedding so it can be used for semantic search\n",
    "from azure.search.documents.indexes.models import SearchIndex\n",
    "from langchain_openai import AzureOpenAIEmbeddings\n",
    "\n",
    "def get_question_embeddings(question_title):\n",
    "    openai_embeddings = AzureOpenAIEmbeddings(\n",
    "        model=\"text-embedding-ada-002\",\n",
    "        chunk_size=1,  # Adjust based on your needs\n",
    "        client=None,\n",
    "        azure_endpoint=\"https://azure-openai-accelerator.openai.azure.com/openai/deployments/text-embedding-ada-002/embeddings?api-version=2023-05-15\",\n",
    "        openai_api_version=\"2023-05-15\",\n",
    "    )\n",
    "    try:\n",
    "        embedding = openai_embeddings.embed_documents(question_title)  # Get the embedding from the list\n",
    "        return embedding  # Explicitly return the embedding\n",
    "    except Exception as e:\n",
    "        print(f\"Error generating embedding for question: {e}\")\n",
    "        return None  # Return None if an error occurs\n",
    "\n",
    "# Example usage\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "ruby"
    }
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Once, offline reinforcement learning is done we will store the self reflection answer in AzureAI Search Index with question being the indexer.\n",
    "\n",
    "'''\n",
    "from azure.search.documents import SearchClient\n",
    "from azure.search.documents.indexes import SearchIndexClient\n",
    "from azure.search.documents.indexes.models import SearchIndex\n",
    "from typing import List\n",
    "from azure.search.documents.indexes.models import (\n",
    "    SearchIndex, SimpleField, SearchField, SearchFieldDataType,\n",
    "    VectorSearch, HnswAlgorithmConfiguration, VectorSearchProfile,\n",
    "    SemanticConfiguration, SemanticPrioritizedFields, SemanticField,\n",
    "    VectorSearchCompression, ScalarQuantizationCompression,  SemanticSearch\n",
    ")\n",
    "\n",
    "# Define function to create a scalar-quantized index without truncation\n",
    "def create_index(index_name, dimensions):\n",
    "    vector_type = \"Collection(Edm.Single)\"\n",
    "\n",
    "    # Define fields for the index\n",
    "    fields = [\n",
    "        SimpleField(name=\"id\", type=SearchFieldDataType.String, key=True, sortable=True, filterable=True),\n",
    "        SearchField(name=\"Question_Title\", type=SearchFieldDataType.String),\n",
    "        SearchField(name=\"generalized_chunk\", type=SearchFieldDataType.String),\n",
    "        SearchField(name=\"question_embedding\", type=vector_type, searchable=True, stored=True, vector_search_dimensions=dimensions, vector_search_profile_name=\"myHnswProfile\")\n",
    "    ]\n",
    "\n",
    "    # Define scalar quantization compression configuration without truncation\n",
    "    compression_name = \"myCompression\"\n",
    "    compression_configurations = [\n",
    "        ScalarQuantizationCompression(compression_name=compression_name)\n",
    "    ]\n",
    "\n",
    "    # Define vector search with compression\n",
    "    vector_search = VectorSearch(\n",
    "        algorithms=[\n",
    "            HnswAlgorithmConfiguration(name=\"myHnsw\")\n",
    "        ],\n",
    "        profiles=[\n",
    "            VectorSearchProfile(name=\"myHnswProfile\", algorithm_configuration_name=\"myHnsw\", compression_name=compression_name)\n",
    "        ],\n",
    "        compressions=compression_configurations\n",
    "    )\n",
    "\n",
    "    # Define semantic configuration\n",
    "    semantic_config = SemanticConfiguration(\n",
    "        name=\"my-semantic-config\",\n",
    "        prioritized_fields=SemanticPrioritizedFields(\n",
    "            title_field=SemanticField(field_name=\"Question_Title\"),\n",
    "            content_fields=[SemanticField(field_name=\"generalized_chunk\")]\n",
    "        )\n",
    "    )\n",
    "    semantic_search = SemanticSearch(configurations=[semantic_config])\n",
    "\n",
    "    return SearchIndex(name=index_name, fields=fields, vector_search=vector_search, semantic_search=semantic_search) \n",
    "\n",
    "# Define index name and dimensions\n",
    "embedding_dimensions = 1536\n",
    "index_name = \"semantic_memory_index\"\n",
    "\n",
    "# Create the SearchIndexClient\n",
    "search_index_client = SearchIndexClient(endpoint=endpoint, credential=credential)\n",
    "\n",
    "# Create the scalar-quantized index without truncation\n",
    "index = create_index(index_name, dimensions=embedding_dimensions)\n",
    "search_index_client.create_or_update_index(index)\n",
    "\n",
    "print(\"Created scalar-quantized index for the semantic_memory_application\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "vscode": {
     "languageId": "ruby"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document with ID 1 inserted: [<azure.search.documents._generated.models._models_py3.IndexingResult object at 0x000001AC8051A940>]\n",
      "Document with ID 2 inserted: [<azure.search.documents._generated.models._models_py3.IndexingResult object at 0x000001AC14229460>]\n",
      "Document with ID 3 inserted: [<azure.search.documents._generated.models._models_py3.IndexingResult object at 0x000001AC14278D00>]\n",
      "Document with ID 4 inserted: [<azure.search.documents._generated.models._models_py3.IndexingResult object at 0x000001AC1423D5E0>]\n",
      "Document with ID 5 inserted: [<azure.search.documents._generated.models._models_py3.IndexingResult object at 0x000001AC1425CA30>]\n",
      "Document with ID 6 inserted: [<azure.search.documents._generated.models._models_py3.IndexingResult object at 0x000001AC14272E80>]\n",
      "Document with ID 7 inserted: [<azure.search.documents._generated.models._models_py3.IndexingResult object at 0x000001AC14282AF0>]\n"
     ]
    }
   ],
   "source": [
    "from azure.search.documents import SearchClient\n",
    "from typing import List\n",
    "\n",
    "# Create the SearchClient to interact with the index\n",
    "search_client = SearchClient(endpoint=endpoint, index_name=index_name, credential=credential)\n",
    "\n",
    "# Function to insert a single document with embedding\n",
    "def insert_single_embedding(embedding: List, question_title: str, generalized_chunk: str, id: str):\n",
    "    document = {\n",
    "        \"id\": id,  # Unique ID for this document\n",
    "        \"Question_Title\": question_title,\n",
    "        \"generalized_chunk\": generalized_chunk,\n",
    "        \"question_embedding\": embedding[0]\n",
    "    }\n",
    "\n",
    "    # Upload the document to the index\n",
    "    result = search_client.upload_documents(documents=[document])\n",
    "    return result\n",
    "\n",
    "# Iterate through the final_log_offline dictionary\n",
    "for key, value in final_log_offline.items():\n",
    "    try:\n",
    "        # Extract question and embedding\n",
    "        question = value.get(\"question\", \"No question provided.\")\n",
    "        embedding = get_question_embeddings([question])  # Replace with actual embedding function\n",
    "        question_title = question\n",
    "        generalized_chunk = value.get(\"answer\", \"No answer provided.\")  # Replace with appropriate value\n",
    "\n",
    "        # Insert the document into the index\n",
    "        result = insert_single_embedding(embedding, question_title, generalized_chunk, key)\n",
    "        print(f\"Document with ID {key} inserted:\", result)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error inserting document with ID {key}: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "ruby"
    }
   },
   "outputs": [],
   "source": [
    "from azure.search.documents import SearchClient\n",
    "\n",
    "# Create the SearchClient to interact with the index\n",
    "question_new = \"Who is the appointed Secretary of Health and Human Services in 2024?\"\n",
    "# Function to perform vector search\n",
    "def search_similar_embeddings(question, k: int = 5):\n",
    "\n",
    "    \n",
    "    # Perform the search\n",
    "    results = search_client.search(\n",
    "        search_text=question,  # Input query for semantic search\n",
    "        top=k,  # Number of top results to return\n",
    "        query_type=\"semantic\",  # Use semantic search\n",
    "        semantic_configuration_name=\"my-semantic-config\",  # Replace with your semantic configuration name\n",
    "        query_caption=\"extractive\",  # Extractive captions for concise explanations\n",
    "        query_caption_highlight_enabled=True,  # Enable highlighting in captions\n",
    "        select=[\"Question_Title\", \"generalized_chunk\"] # Empty search text because we're using vector search\n",
    "    )\n",
    "    \n",
    "    # Return the results\n",
    "    return results\n",
    "\n",
    "# Example query embedding\n",
    "\n",
    "# Perform the search\n",
    "generalized_chunk_semantic_memory =\"\"\n",
    "results = search_similar_embeddings(question_new, k=2)\n",
    "for result in results:\n",
    "    print(question_new)\n",
    "    generalized_chunk_semantic_memory = result[\"generalized_chunk\"]\n",
    "    \n",
    "print(generalized_chunk_semantic_memory)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "vscode": {
     "languageId": "ruby"
    }
   },
   "outputs": [],
   "source": [
    "SEMANTIC_MEMORY_PROMPT =\"\"\"You are a helpful agent. You will get a question and a generalized text on how you should approach the problem. Make sure to take the generalized text into consideration BUT DO NOT COMPLETELY RELY ON IT. Using the generalized approach, answer the given query and present the user with a comprehensive answer. Question: {new_question} and the generalized approach to follow: {results}\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "ruby"
    }
   },
   "outputs": [],
   "source": [
    "##Adding the semantic memory to the ReACT Agent\n",
    "semantic_memory_prompt = SEMANTIC_MEMORY_PROMPT.format(new_question = question_new, results = generalized_chunk_semantic_memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "ruby"
    }
   },
   "outputs": [],
   "source": [
    "# Initial attempt to invoke the agent with ReACT thought process\n",
    "output = None\n",
    "while output is None:\n",
    "    try:\n",
    "        output = agent_executor.invoke({\"input\": semantic_memory_prompt})\n",
    "    except Exception as e:\n",
    "        print(f\"Error occurred: {e}. Retrying...\")\n",
    "\n",
    "# Display the initial answer\n",
    "print(\"Agent's answer:\", output[\"output\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "ruby"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
